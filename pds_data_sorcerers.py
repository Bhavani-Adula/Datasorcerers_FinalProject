# -*- coding: utf-8 -*-
"""PDS_DATA_SORCERERS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V8lIZSg-vvkykJTm3GCAWVl3uLhtT3wD

# **CUSTOMER CHURN PREDICTION USING LLMs WITH SHAP AND BEHAVIOR SUMMARIES:**

Import libraries:
"""

import pandas as pd
import numpy as np
import random
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from torch.utils.data import Dataset
import torch

# Load and Inspect Data
df = pd.read_csv("/content/drive/MyDrive/PDS_FINAL/Customer_churn.csv")
original_df = df.copy()

original_df = df.copy()

# Initial inspection
print("Initial shape:", df.shape)
print("\nData Types:\n", df.dtypes)
print("\nMissing values:\n", df.isnull().sum())
print("\nUnique values in Churn column:", df['Churn'].unique())

# Data Cleaning
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())
df.drop(columns=['customerID'], inplace=True)
df['Churn'] = df['Churn'].map({'No': 'no', 'Yes': 'yes'})

"""EDA visualizations"""

px.pie(df, names='Churn', title='Churn Distribution').show()
px.histogram(df, x='MonthlyCharges', color='Churn', marginal='box', nbins=50, title='Monthly Charges by Churn').show()
px.box(df, x='Contract', y='TotalCharges', color='Churn', title='Total Charges by Contract Type and Churn').show()
px.bar(df.groupby(['InternetService', 'Churn']).size().reset_index(name='count'),
       x='InternetService', y='count', color='Churn', barmode='group', title='Churn by Internet Service').show()
px.imshow(df[['tenure', 'MonthlyCharges', 'TotalCharges']].corr(), text_auto=True, title='Correlation Heatmap').show()

from transformers import (
    T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,
    BertTokenizer, BertForSequenceClassification
)

!pip install --upgrade transformers

""" BERT-Based Classification

"""

df_bert = df.copy()
df_bert['Churn_Label'] = df_bert['Churn'].map({'no': 0, 'yes': 1})

def row_to_prompt(row):
    return (
        f"{row['gender']} customer, {'Senior' if row['SeniorCitizen'] else 'Non-senior'}, "
        f"Contract: {row['Contract']}, MonthlyCharges: ${row['MonthlyCharges']}, "
        f"InternetService: {row['InternetService']}, TechSupport: {row['TechSupport']}, "
        f"TotalCharges: ${row['TotalCharges']}"
    )

df_bert['text'] = df_bert.apply(row_to_prompt, axis=1)
X_train_bert, X_val_bert, y_train_bert, y_val_bert = train_test_split(
    df_bert['text'], df_bert['Churn_Label'], test_size=0.2, stratify=df_bert['Churn_Label'], random_state=42
)

bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class BertChurnDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = bert_tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128)
        self.labels = labels.tolist()
    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),
            'labels': torch.tensor(self.labels[idx])
        }
    def __len__(self): return len(self.labels)

train_dataset_bert = BertChurnDataset(X_train_bert, y_train_bert)
val_dataset_bert = BertChurnDataset(X_val_bert, y_val_bert)

bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

bert_args = TrainingArguments(
    output_dir='./bert_results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir='./bert_logs',
    logging_steps=10
)

bert_trainer = Trainer(
    model=bert_model,
    args=bert_args,
    train_dataset=train_dataset_bert,
    eval_dataset=val_dataset_bert,
    tokenizer=bert_tokenizer
)

bert_trainer.train()

bert_preds_output = bert_trainer.predict(val_dataset_bert)
bert_preds = np.argmax(bert_preds_output.predictions, axis=1)

print("BERT Accuracy:", accuracy_score(y_val_bert, bert_preds))
print("\nBERT Classification Report:\n", classification_report(y_val_bert, bert_preds))

# Convert numeric predictions (0/1) to labels (no/yes)
bert_pred_labels = ['yes' if pred == 1 else 'no' for pred in bert_preds]
actual_labels = ['yes' if y == 1 else 'no' for y in y_val_bert]

# first 20 entries only
bert_results_df = pd.DataFrame({
    'Text Prompt': X_val_bert.values[:20],
    'Actual Churn': actual_labels[:20],
    'Predicted Churn': bert_pred_labels[:20]
})


print("ðŸ“‹ First 20 BERT Classification Results:")
display(bert_results_df)

# -------------------- Convert to Natural Language Prompt --------------------
def row_to_text(row):
    return (
        f"A {row['gender'].lower()} customer, "
        f"{'a senior' if row['SeniorCitizen'] else 'not a senior'}, "
        f"on a {row['Contract'].lower()} contract, paying ${row['MonthlyCharges']} per month, "
        f"uses {row['InternetService'].lower()} internet, {'has' if row['TechSupport'] == 'Yes' else 'no'} tech support, "
        f"total charges ${row['TotalCharges']}. Customer support says: '{row['SupportInteraction']}' "
        f"(Sentiment: {row['Sentiment']})."
    )

df['text'] = df.apply(row_to_text, axis=1)
df['label'] = df['Churn']

# -------------------- Train-Test Split --------------------
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42, stratify=df['label']
)

"""T5 Model Setup"""

#Tokenizer and Model
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

class ChurnDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors="pt")
        self.labels = tokenizer(labels, padding=True, truncation=True, max_length=10, return_tensors="pt")
    def __len__(self): return len(self.inputs['input_ids'])
    def __getitem__(self, idx):
        return {
            'input_ids': self.inputs['input_ids'][idx],
            'attention_mask': self.inputs['attention_mask'][idx],
            'labels': self.labels['input_ids'][idx]
        }

train_dataset = ChurnDataset(train_texts, train_labels, tokenizer)
val_dataset = ChurnDataset(val_texts, val_labels, tokenizer)

args = TrainingArguments(
    output_dir='./results', num_train_epochs=3,
    per_device_train_batch_size=8, per_device_eval_batch_size=8,
    logging_dir='./logs', logging_steps=10
)

trainer = Trainer(model=model, args=args, train_dataset=train_dataset, eval_dataset=val_dataset)

#Train Model
trainer.train()

#Evaluate Model
# Tokenize validation texts
val_inputs = tokenizer(val_texts, return_tensors="pt", padding=True, truncation=True).to(model.device)

# Generate predictions
generated_ids = model.generate(
    input_ids=val_inputs["input_ids"],
    attention_mask=val_inputs["attention_mask"],
    max_length=10
)

# Decode predictions
preds_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

# Evaluate
print("Accuracy:", accuracy_score(val_labels, preds_text))
print(classification_report(val_labels, preds_text))

# Confusion Matrix
cm = confusion_matrix(val_labels, preds_text, labels=['no', 'yes'])
fig_cm = go.Figure(data=go.Heatmap(
    z=cm, x=['Predicted No', 'Predicted Yes'], y=['Actual No', 'Actual Yes'],
    colorscale='Blues', text=cm, texttemplate="%{text}"
))

"""SHAP + XGBoost"""

# SHAP Explainability with XGBoost

import shap
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Prepare tabular data (exclude text columns used for T5)
df_xgb = df.copy()

# Encode categorical columns
categorical_cols = df_xgb.select_dtypes(include=['object']).columns.tolist()
categorical_cols.remove("Churn")  # target column

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_xgb[col] = le.fit_transform(df_xgb[col])
    label_encoders[col] = le

# Drop only columns that exist
columns_to_drop = [col for col in ["Churn", "text", "label", "summary", "customerID", "SupportInteraction"] if col in df_xgb.columns]
X = df_xgb.drop(columns=columns_to_drop)
y = df_xgb["Churn"].map({'no': 0, 'yes': 1})

# Train XGBoost model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Apply SHAP
explainer = shap.Explainer(xgb_model, X_train)
shap_values = explainer(X_test)

# Global feature importance
shap.summary_plot(shap_values, X_test)

import os
import random

#Simulated Support Interaction Phrases
support_phrases = [
    "Internet keeps disconnecting. Not happy.",
    "Billing was incorrect last month.",
    "Great service, very satisfied.",
    "Slow speeds during evenings.",
    "Excellent customer support. No complaints.",
    "I want to cancel if the issue persists.",
    "Everything works fine. Happy so far.",
    "Repeated problems with my modem.",
    "Quick resolution to my last issue.",
    "No issues. Just checking my bill."
]
df["SupportInteraction"] = [random.choice(support_phrases) for _ in range(len(df))]



def analyze_sentiment(text):
    text = text.lower()
    if any(word in text for word in ["not happy", "cancel", "problems", "slow", "incorrect"]):
        return "Negative"
    elif any(word in text for word in ["great", "excellent", "happy", "fine", "no issues", "quick resolution"]):
        return "Positive"
    else:
        return "Neutral"

df["Sentiment"] = df["SupportInteraction"].apply(analyze_sentiment)

# Show 10 sample rows with support interaction and sentiment
print(" Sample Support Interactions and Sentiments:\n")
display(df[['customerID', 'SupportInteraction', 'Sentiment']].head(10))

# Count how many are Positive, Negative, Neutral
print("Sentiment Distribution:\n")
print(df['Sentiment'].value_counts())

df['customerID'] = original_df['customerID']

def generate_summary(row):
    sentiment = row['Sentiment']
    actual = row['Churn']

    if sentiment == "Negative":
        risk = "HIGH"
    elif sentiment == "Neutral":
        risk = "MODERATE"
    else:
        risk = "LOW"

    return (
        f"Customer ID: {row['customerID']}. A {row['gender'].lower()} customer, "
        f"{'senior' if row['SeniorCitizen'] else 'non-senior'}, uses {row['InternetService'].lower()} internet "
        f"with {row['Contract'].lower()} contract. Pays ${row['MonthlyCharges']} monthly, "
        f"total charges ${row['TotalCharges']}. Sentiment from support: {sentiment}. "
        f"Risk of churn (based on sentiment): {risk}. Actual churn: {actual.upper()}."
    )

df['summary'] = df.apply(generate_summary, axis=1)

# Step 1: Make sure customerID is in the main DataFrame
df['customerID'] = original_df['customerID']

# Step 2: Apply the summary generation function to every row
df['summary'] = df.apply(generate_summary, axis=1)

# Step 3: Display a few example summaries
print(" Sample Customer Summaries:\n")
for summary in df['summary'].head(5):
    print(summary)
    print("-" * 100)

df[['customerID', 'summary']].to_csv("customer_summaries_with_sentiment.csv", index=False)
print(" Summaries saved to 'customer_summaries_with_sentiment.csv'")

df[['customerID', 'summary']].to_csv("/content/customer_summaries_with_sentiment.csv", index=False)

from google.colab import files
files.download("/content/customer_summaries_with_sentiment.csv")